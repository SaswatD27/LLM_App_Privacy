Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:06<00:06,  6.33s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  3.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:08<00:00,  4.31s/it]
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.
  0%|          | 0/5000 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  0%|          | 0/5000 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/sw/ubuntu-22.04/anaconda3/2023.03/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/sw/ubuntu-22.04/anaconda3/2023.03/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/bigtemp/duh6ae/LLM_App_Privacy/src/tests/adult_utility_w_regexpostdefense_w_safety_prompt.py", line 18, in <module>
    response = app_agent.run(input_text)
  File "/bigtemp/duh6ae/LLM_App_Privacy/src/utils/agentUtils.py", line 137, in run
    return self.postdefense(response)
  File "/bigtemp/duh6ae/LLM_App_Privacy/src/utils/agentUtils.py", line 56, in postdefense
    response = defense(self, response)
  File "/bigtemp/duh6ae/LLM_App_Privacy/src/utils/defense/defenseUtils.py", line 11, in check_jailbroken
    private_tokens.append(agent.context.lower().split(f'{att} is ')[1].split('.')[0])
IndexError: list index out of range
Fetch Label Logits?: True
Fetching Label Logits!
generated_tokens:tensor([[694,   2]], device='cuda:0')
|   694 | no       | -0.8848 | 41.28%
